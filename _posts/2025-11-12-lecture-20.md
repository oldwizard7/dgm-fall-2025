---
layout: distill
title: Lecture 20
description: Attention and Transformers
date: 2025-11-12

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name: Greg Adler # author's full name
  - name: Kunal Singh

editors:
  - name:

---

# Attention and Transformer Architecture

## The Attention Mechanism

**Motivation:** Different parts of our input relate to different parts of our output. Sometimes these important relationships can be far apart, like in machine translation. Attention helps us dynamically calculate what is important.

* **Origin:** Originally from Natural Language Processing (NLP) and language translation.
* **Core Idea:** Assign an attention weight to each word to signal how much we should "attend" to that word.
* **Evolution:** We later learned that attention and recurrence are not both necessary, popularized by the paper *"Attention is all you need"*. <d-cite key="attentionIsAllYouNeed"></d-cite>



### Hard Attention
Hard attention makes a binary **0/1 decision** about where to attend. It asks: *"Is this input important to this prediction or not?"*

* **Pros:** Easy to interpret.
* **Cons:** Difficult to train; requires reinforcement learning. It is rarely used in practice.

### Soft Attention
Rather than a binary decision, we assign a continuous weight between **0 and 1** for each input.

* **Mechanism:** A softmax function is used to compute weights, ensuring they are real values between 0 and 1 and that the sum of all weights equals 1.
* **Interpretation:** Each weight represents the proportion of influence that input has on the current prediction.
* **Visualization:** in Machine Translation, visualizing the influence of inputs on outputs usually shows a strong diagonal (token-to-token), except where word order differs (e.g., French vs. English nouns).



#### Soft Attention vs. RNN for Image Captioning

**RNNs:**
* Only look at the image (or the 1-D CNN embedding of the original image) **once** to try to find important features.
* Successively refer back to the *same* hidden state, which must encode information about both foreground and background.

**Soft Attention:**
* Generates an **attention map**, which refers back to a 2-D CNN embedding at *each* hidden state.
* Generates a word and an input to the next hidden state.
* Keeps looping back to the input, tying weights to a combination of learnable features. Allows each word in the caption to refer to different parts of the image.

> **Aside:** CNNs were an example of **Hard Attention**. As the filter slides over the image, the part of the image inside the filter gets attention weight 1, and the rest gets weight 0.

---

## Self Attention

**Motivation:** Can we get rid of the sequential RNN component? Since attention already ties inputs across the sequence, is it necessary to continue to loop over it?

### Basic Self Attention
**Main procedure:**
1.  **Derive Attention Weights:** Calculate similarity between each current input and all other inputs.
2.  **Normalize:** Use softmax to normalize weights.
3.  **Compute:** Calculate attention from normalized weights and corresponding inputs.

**Computing Weights:**
We use a dot product when computing attention weights. Dot products are similar to cosine similarity, but are sensitive to vector magnitudes. Since we will be learning weights (not in this basic version, but later), there is no need to normalize in practice. 
* The self-attention corresponding to the `$i$`-th input, `$x_i$`, is `$A_i$`.
* `$A_i$` is the weighted sum over all inputs, where the weight for `$x_j$` is `$a_{i,j}$`.
* `$a_{i,j}$` is the softmax output for the dot product between `$x_i$` and `$x_j$`.


<img src="{{ '/assets/img/notes/lecture-20/BasicSelfAttention.png' | relative_url }}" />


### Learnable Self Attention
The basic version has no learnable parameters. To fix this, we add three trainable weight matrices to be multiplied by input sequence embeddings: **Query**, **Key**, and **Value**.


1.  **Query (`$Q$`):** Represents what the token is "asking" the rest of the sequence.
2.  **Key (`$K$`):** Describes how the token "advertises" the information it holds.
3.  **Value (`$V$`):** The actual content shared if other tokens pay attention to it.

**The Process:**
For every token, the model compares its **Query** to the **Keys** of all tokens in the sequence.
* **High Alignment:** If query/key align well `$\rightarrow$` High attention score.
* **Low Alignment:** If not `$\rightarrow$` Low attention score.

These scores are normalized to act as weights. Each token builds a new representation by taking a weighted blend of the **Value** vectors from all tokens. Every position becomes a learned mixture of information pulled from everywhere else, with the mixing proportions determined by how relevant the model thinks each other token is.


<img src="{{ '/assets/img/notes/lecture-20/LearnableSelfAttention.png' | relative_url }}" />


> **Why this works:** Because `$Q$`, `$K$`, and `$V$` are trainable, the model learns its own notion of "relevant context." Early layers may focus on nearby words; deeper layers may learn to link pronouns to nouns or relate the start/end of sentences. This structure emerges from adjusting those weight matrices to reduce training loss, turning self-attention into a flexible, learned mechanism for combining information across a sequence.

---

## The Transformer

Originally proposed for machine translation. It consists of two stacks side-by-side, each replicated `$N$` times.



<img src="{{ '/assets/img/notes/lecture-20/TransformerArchitecture.png' | relative_url }}" />


* **Left Stack:** The input stack. This is the only stack used in GPT models.
* **Right Stack:** The output stack.

### Components
* **Multi-headed Attention:** Applies self-attention multiple times in parallel. `$Q$`, `$K$`, and `$V$` matrices have multiple rows, allowing the model to attend to different parts of the sequence differently simultaneously.
* **Skip Connections (Residuals):** We add `Input + (Input passed through attention layer)`. This helps manage vanishing gradients.

### Transformer Tricks (NLP)
* **Self-Attention:** Each layer combines words with others.
* **Multi-headed Attention:** 8 attention heads learned independently.
* **Normalized Dot-Product Attention:** Removes bias in the dot product when using large networks.
* **Positional Encodings:** Ensures that even without an RNN, the model can distinguish positions.
    * *Note: This may not matter as much as thought, as the attention mechanism can generate position-specific encodings itself.*

---

<footer>
<p>© 2025 University of Wisconsin — STAT 453 Lecture Notes</p>
</footer>
