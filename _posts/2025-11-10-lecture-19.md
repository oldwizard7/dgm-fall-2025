---
layout: distill
title: "Lecture 19"
description: "Sequence Learning with RNNs" 
date: 2025-11-10

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name:  Yi Zhang
  - name:  Yufan Cheng
  - name:  Erica Henderson
---
## Motivation: Why Text Modeling Is Challenging

Modeling text sequences is difficult due to:

- **Variable sequence length**
- **Long-range dependencies**
- **Need for generative models**
- **Large data requirements**

## Historical Approaches

### Bag-of-Words (BOW)
Represents text as unordered word counts.

**Problem:** Word order is lost  
Example:  
- “NOT good”  
- “good NOT”  
become identical vectors.

### Hidden Markov Models (HMMs)
Use the Markov assumption:
\[
P(Y_n \mid X_1,\ldots,X_n) = P(Y_n \mid X_n)
\]

**Problem:** Only captures one-step dependencies.

### Convolutional Neural Networks (CNNs)
Capture local patterns using filters.

**Limitation:** Require fixed-length input; cannot naturally handle long sequences.

---

## Recurrent Neural Networks

RNNs address sequential dependence using recurrent connections.

### The Recurrence Equation

\[
h^{(t)} = \sigma(W_{hx} x^{(t)} + W_{hh} h^{(t-1)} + b_h)
\]

Where:

- \(h^{(t)}\): Hidden state  
- \(x^{(t)}\): Input at time \(t\)  
- \(W_{hx}, W_{hh}\): Input and recurrent weights  
- \(\sigma\): Typically `tanh`

<figure>
  <img src="{{ '/assets/img/notes/lecture-19/singlelayerrrn2.jpeg' | relative_url }}" style="width:80%; margin:auto;">
  <figcaption><strong>Figure 1.</strong> Unrolled RNN: each hidden unit receives input and the previous hidden state.</figcaption>
</figure>

### Unrolling Through Time

Unrolling converts an RNN into a deep network with depth equal to sequence length.

<figure>
  <img src="{{ '/assets/img/notes/lecture-19/multilayerrrn.jpeg' | relative_url }}" style="width:80%; margin:auto;">
  <figcaption><strong>Figure 2.</strong> Multilayer RNN with stacked hidden states.</figcaption>
</figure>

### RNN Architectures

- **Many-to-One:** sentiment classification  
- **One-to-Many:** image captioning  
- **Many-to-Many:**  
  - synchronous: video captioning  
  - delayed: machine translation

<figure>
  <img src="{{ '/assets/img/notes/lecture-19/weighmatrixrrn.jpeg' | relative_url }}" style="width:80%; margin:auto;">
  <figcaption><strong>Figure 3.</strong> RNN input–output configurations.</figcaption>
</figure>

---

## Training RNNs: Backpropagation Through Time (BPTT)

Training RNNs involves unrolling through time and applying the chain rule.

### Gradient Expression

\[
\frac{\partial L^{(t)}}{\partial W_{hh}} =
\frac{\partial L^{(t)}}{\partial y^{(t)}}
\frac{\partial y^{(t)}}{\partial h^{(t)}}
\left(
\sum_{k=1}^t
\frac{\partial h^{(t)}}{\partial h^{(k)}}
\frac{\partial h^{(k)}}{\partial W_{hh}}
\right)
\]

The key term is:

\[
\frac{\partial h^{(t)}}{\partial h^{(k)}}
=
\prod_{i=k+1}^t
\frac{\partial h^{(i)}}{\partial h^{(i-1)}}
\]

A product of many Jacobians → unstable.

### Vanishing and Exploding Gradients

- If Jacobians < 1 → **vanishing gradients**  
- If Jacobians > 1 → **exploding gradients**

Consequences:

- Loss of long-range memory  
- Training instability

### Remedies

- **Gradient clipping**  
- **Truncated BPTT**  
- **LSTM / GRU architectures**

---

## Long Short-Term Memory (LSTM)

LSTMs solve gradient issues by introducing a **cell state** \(C_t\) that acts as a memory highway.

### LSTM Gate Equations

\[
\begin{aligned}
f_t &= \sigma(W_f[x_t, h_{t-1}] + b_f) \\
i_t &= \sigma(W_i[x_t, h_{t-1}] + b_i) \\
g_t &= \tanh(W_g[x_t, h_{t-1}] + b_g) \\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t \\
o_t &= \sigma(W_o[x_t, h_{t-1}] + b_o) \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}
\]

### Interpretation

- **Forget gate:** remove irrelevant memory  
- **Input gate + candidate:** add new information  
- **Output gate:** expose memory to next layer  
- **Cell state:** preserves long-term gradients

---

## Preparing Text for RNNs

### Step 1 — Build Vocabulary
Include: `<unk>`, `<pad>`, `<bos>`, `<eos>`.

### Step 2 — Convert Text to Indices
Pad sequences to equal length for batching.

### Step 3 — One-Hot Encoding (conceptual)
Not used in modern practice due to high dimensionality.

### Step 4 — Word Embeddings

Learned dense vectors:

- Low dimension (e.g., 300-D)
- Similar words → similar embeddings
- Implemented via `nn.Embedding`

---

## RNNs for Generative Modeling

RNNs model the autoregressive distribution:

\[
P_\theta(X) = \prod_t P_\theta(X_t \mid X_{<t})
\]

Training objective:

\[
\max_\theta \sum_i \sum_t \log P_\theta(X_{i,t} \mid X_{i,<t})
\]

Used in classic language modeling.

---

## Many-to-One Word RNN Example

1. Build vocabulary  
2. Convert text → indices  
3. Convert indices → embeddings  
4. Feed sequence into RNN  
5. Use final hidden state for classification

---

## Summary

| Concept | Description |
|--------|-------------|
| RNN | Maintains hidden state to model sequential data |
| BPTT | Unrolls through time; gradient instability |
| LSTM | Gates + memory cell enable long-term learning |
| Gradient Clipping | Prevents gradient explosion |
| Truncated BPTT | Stabilizes training by limiting depth |
| Embeddings | Dense learned word representations |
| Applications | NLP, speech, translation, time series |
