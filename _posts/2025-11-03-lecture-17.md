---
layout: distill
title: "Lecture 17"
description: "Generative Adversarial Networks (GANs)"
lecturers:
  - "Ben Lengerich"
authors:
  - "Erica Henderson"
  - "Ernie Dippold"
  - "Aashit Paliwal"
  - "Pranshu Dewagan"
editors:
  -
date: 2025-11-03
permalink: /notes/lecture-17/
bibliography: /assets/bibliography/2025-11-03-lecture-17.bib
---
## November 3 — Generative Adversarial Networks (GANs)

### Topics

1. Review: Autoencoders
2. Generative Adversarial Networks (GANs)
3. GANs and VAEs: A Unified View

---

## 1. Autoencoders (Review)

**Goal:** Learn a compressed latent representation of input ( x ).

**Structure:**
$
\hat{x} = f(h) = f(g(x))
$
where:

* ( $g$ ): encoder
* ( $f$ ): decoder

### Variants

#### Denoising Autoencoders

* Add noise (e.g., dropout or Gaussian noise) to input.
* Train to reconstruct the original, uncorrupted input.
* Purpose: Learn robust representations that can remove noise.

#### Autoencoders with Dropout

* Dropout layers encourage redundancy in learned features.
* Improves generalization and robustness to missing inputs.

#### Sparse Autoencoders

* Loss function:
$
L = \lVert x - \hat{x} \rVert^2 + \lambda \sum_i |h_i|
$

* Adds an L1 penalty on activations to enforce sparsity.
* Produces interpretable features — each neuron learns a distinct factor.

#### Variational Autoencoders (VAEs)

* Latent variable ( $z \sim \mathcal{N}(0, I)$ )
* Enables sampling new data points.
* Provides a probabilistic framework for generative modeling.

---
## 2. Generative Adversarial Networks (GANs)

## Overview

Generative Adversarial Networks (GANs) were introduced by Goodfellow et al. (2014), and is a generative modeling framework between a **generator** that produces synthetic samples and a **discriminator** that tries to distinguish them from real data. Unlike autoencoders or autoregressive models, GANs can generate an entire sample with less steps.

---

## Motivation

Traditional generative models (Gussian Mixtures, VAEs, and autoencoders) often struggle to uncover the complexity of high-dimensional data like that of images. GANs are flexible and capable of learning **implicit data distributions**. This makes them useful for image synthesis, style transfer, and text to image applications like those developed by OpenAI (DALL-E 2) and Google (Imagen).

---

## Architecture and Training

### Generator ($G_\theta$)

- Maps a noise vector $z \sim p(z)$ (often Normal(0, I)) to a data-space sample $x = G_\theta(z)$.
- Goal: produce samples indistinguishable from real data $x \sim p_{data}$.
- Fool the discriminator (increase $D(G(z))$)
- Trained via gradient descent to maximize $D(G(z))$.

<figure>
  <img src="{{ '/assets/img/notes/lecture-17/gandescent.png' | relative_url }}" alt="Gradient Descent with GAN" style="width:80%; display:block; margin:auto;" />
  <figcaption><strong>Figure 1.</strong> The adversarial training loop between generator and discriminator.</figcaption>
</figure>

### Discriminator ($D_\phi$)

- Binary classifier outputting $D_\phi(x)\in[0,1]$, - Trained via gradient ascent to maximize $\log D(x)$ for real data
  and minimize $\log (1 - D(G(z)))$ for fake data.
- Gradient update: Gradient _ascent_

<figure>
  <img src="{{ '/assets/img/notes/lecture-17/ganascent.png' | relative_url }}" alt="Gradient Ascent with GAN" style="width:80%; display:block; margin:auto;" />
  <figcaption><strong>Figure 1.</strong> The adversarial training loop between generator and discriminator.</figcaption>
</figure>

### Minimax Objective

The generator minimizes this value by making its outputs hard to distinguish,
while the discriminator maximizes it by improving classification.

<figure>
  <img src="{{ '/assets/img/notes/lecture-17/gantraining.png' | relative_url }}" alt="Discriminator vs. Generator Training" style="width:80%; display:block; margin:auto;" />
  <figcaption><strong>Figure 1.</strong> The adversarial training loop between generator and discriminator.</figcaption>
</figure>

---

## Training Characteristics

Training alternates between both networks. Convergence ideally occurs at **Nash Equilibrium**, where the generator’s distribution equals the true data distribution and the discriminator outputs 0.5 for all inputs.

In practice training is often unstable:

- Oscillatory losses between G and D
- **Mode collapse** (one prototype sample repeated)
- **Vanishing gradients** if D dominates this can lead to non-saturating loss being recommended
- **Hyperparameter sensitivity** (learning rate, architecture, batch size)

---

## Interpretations and Variants

A pure equilibrium may not exist, which explains observed oscillations and non-convergence.

### Deep Convolutional GAN (DC-GAN)

Introduces convolutional architectures to stabilize training and capture spatial features.

<figure>
  <img src="{{ '/assets/img/notes/lecture-17/dcgan.png' | relative_url }}" alt="Deep Convolutional GAN" style="width:80%; display:block; margin:auto;" />
  <figcaption><strong>Figure 1.</strong> The adversarial training loop between generator and discriminator.</figcaption>
</figure>

---

## 3. GANs and VAEs: A Unified View

### A Unified View

| Feature         | Autoencoders (AEs)           | Variational Autoencoders (VAEs)  | GANs                         |
| --------------- | ---------------------------- | -------------------------------- | ---------------------------- |
| Goal            | Learn latent representations | Probabilistic generative model   | Adversarial generative model |
| Latent Variable | Deterministic ( $h = g(x)$ )   | ( $z \sim \mathcal{N}(0, I)$ )     | ( $z \sim p_z(z)$ )            |
| Training        | Reconstruction loss          | ELBO (KL + reconstruction)       | Adversarial minimax loss     |
| Sampling        | Deterministic decode         | Random sampling via latent prior | Generator sampling ( G(z) )  |
| Weakness        | Not generative               | Blurry outputs                   | Instability in training      |

### VAEs vs. GANs: a cloesup

| Aspect              | Variational Autoencoder (VAE) | Generative Adversarial Network (GAN)             |
| ------------------- | ----------------------------- | ------------------------------------------------ |
| **Objective**       | Single ELBO maximization      | Two opposing objectives ($\min_G$, $\max_D$)     |
| **Regularization**  | KL term via prior $p(z)$      | Implicit regularization via adversarial feedback |
| **Inference Model** | $q_\phi(z \mid x)$            | $p_\theta(x \mid y)$, $q_\phi(y \mid x)$         |
| **Generation**      | Explicit probability model    | Implicit distribution (no likelihood)            |

<figure>
  <img src="{{ '/assets/img/notes/lecture-17/ganvsvan.png' | relative_url }}" alt="GAN vs. VAE" style="width:80%; display:block; margin:auto;" />
  <figcaption><strong>Figure 1.</strong> The adversarial training loop between generator and discriminator.</figcaption>
</figure>

GANs can be expressed in a variational-EM-like framework:

- **E-step:** update discriminator to approximate $q_\phi(y \mid x)$
- **M-step:** update generator to improve $p_\theta(x \mid y)$

---

## Common Problems

| Problem                    | Explanation                         | Typical Fixes                                        |
| -------------------------- | ----------------------------------- | ---------------------------------------------------- |
| **Mode Collapse**          | Generator outputs few modes of data | Mini-batch discrimination, WGAN, feature matching    |
| **Vanishing Gradient**     | Discriminator too strong            | Non-saturating loss (maximize $\log D(G(z))$)        |
| **Training Oscillation**   | No stable equilibrium               | Gradient penalty, slow updates, learning-rate tuning |
| **Over-fit Discriminator** | Memorizes training data             | Dropout, label smoothing                             |

Empirically, GANs generalize only when the discriminator capacity and training data are balanced.  
Otherwise, Jensen–Shannon and Wasserstein divergence analyses can be misleading in finite settings.

---

### References

- Goodfellow et al. (2014) _Generative Adversarial Nets_ (NIPS).
- Arora & Hardt (2017) _Generative Adversarial Networks: Some Open Questions_ (OffConvex Blog).
- Radford et al. (2015) _Unsupervised Representation Learning with Deep Convolutional GANs_.
- Hu et al. (2017) _Unifying Deep Generative Models_.